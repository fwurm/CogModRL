{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397eb8a9-84a2-488e-8481-f4b58481f28d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reinforcement Learning \n",
    "A computational model of reinforcement learning for a simple Gridworld Problem\n",
    "\n",
    "```\n",
    "  ____    ____     ____   ____\n",
    "  ____    ____     ____   ____\n",
    "\n",
    "|| s00  |  s01  |  s02 | s03 ||\n",
    "  ____    ____     ____   ____\n",
    "    \n",
    "|| s04  |  s05  |  s06 | s07 ||\n",
    "  ____    ____     ____   ____\n",
    "  \n",
    "|| s08  |  s09  |  s10 | s11 ||\n",
    "  ____    ____     ____   ____\n",
    "   \n",
    "|| s12  |  s13  |  s14 | s15 ||\n",
    "  ____    ____     ____   ____\n",
    "  ____    ____     ____   ____\n",
    "```\n",
    "\n",
    " - Agent starts in s0 and can chose from 4 different actions (up, down, left, right).\n",
    " \n",
    " - Reward is only delivered in final state 15.\n",
    " \n",
    " - || (horizontal & vertical) indicates non-permeable wall\n",
    " - | (horizontal & vertical) indicates permeable wall\n",
    " \n",
    " <br>\n",
    " <br>\n",
    " Course: Cognitive Modelling (4032COGNMY)  <br>\n",
    " Author: Franz Wurm <f.r.wurm@fsw.leidenuniv.nl> <br>\n",
    " Date: Sept. 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299393b5-5465-4930-9db0-d00d9595e84a",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "In a first step we should load important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52a653-3f38-4c39-a68a-5d58280d03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ??? as ?\n",
    "from ??? import ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4d93e-882f-452f-bcff-294747d36bc4",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "\n",
    "Here we set the framework for our simulation and the agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30c595-8716-4fbb-8ded-7f83bad39812",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 50 #specify how many times you want to run the q-learning function from start to end\n",
    "\n",
    "method = ??? #choose from multiple options for the decision rule\n",
    "\n",
    "additional parameters = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e09cfa-4e8c-4c46-80c1-ef92091daf7f",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "\n",
    "Here we define important variables for our simulation (e.g. the start and goal state of the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7169847-4520-4354-8631-ff2737453bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(['up', 'left', 'down', 'right']) #possible actions in each state\n",
    "\n",
    "s_0 = ? #start state\n",
    "s_terminal = ? #final state (goal)\n",
    "\n",
    "\n",
    "iZ = 0\n",
    "states= np.empty([4,4],dtype=float)\n",
    "for iY in range(4):\n",
    "    for iX in range(4):\n",
    "        states[iY][iX] = iZ\n",
    "        iZ +=1\n",
    "print('states')\n",
    "print(states)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28764b4-ae33-4edc-a601-d3a1c523ba1d",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "\n",
    "Some computational steps have to be performed multiple times, i.e. on every trial.\n",
    "Here we define functions so we can call them later on in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157acc0-9346-4240-a3c6-8028f1f689fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move1step(var1, var2):\n",
    "    \n",
    "    #apply rules so that you can derive the next state from the input variable(s)\n",
    "    \n",
    "    return(next_state)  \n",
    "\n",
    "def getReward(var1):\n",
    "    \n",
    "    #apply rules so that you can derive a reward from the input variable(s)\n",
    "    \n",
    "    return reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fda619-f539-444a-95c9-5db07a7380a0",
   "metadata": {},
   "source": [
    "### Start of simulation\n",
    "\n",
    "Now that we have defined the agent and the environment, we can start the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f8b03-7b9a-48c0-99bf-e616bd2b8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Qmatrix (=expectations for each state-action pair)\n",
    "Q = np.zeros([?what should be the dimensions here?] )\n",
    "\n",
    "\n",
    "for iT in range(nTrial): # loop for the different runs\n",
    "    state = s_0 # start in initial state as defined previously\n",
    "    while (state ???): # loop within one run\n",
    "        qvals = Q[???] # load qvalue for the current state\n",
    "              \n",
    "        # select action using choice rules\n",
    "        if (method == 'softmax'):\n",
    "            pvals = ??? # convert qvalues into probabilities\n",
    "            action = np.random.choice(actions,size = 1, p = pvals.flatten()) # sample from  actions with specific probabilities          \n",
    "        elif (method == ???):\n",
    "            #There are multiple alternatives to the softmax decision rule\n",
    "            #helpful function could be: numpy.max() or np.random.random()\n",
    "        else:\n",
    "            ???\n",
    "        \n",
    "        # interact with environment\n",
    "        # here we can use the function we defined earlier\n",
    "        next_state = move1step(?inputvariables?)\n",
    "        reward = getReward(?inputvariables?)\n",
    "        \n",
    "        actionidx = (action==actions)\n",
    "        coord1 = np.where(states == next_state) \n",
    "        \n",
    "        # update expectations using learing rules\n",
    "        # a few things are stil missing\n",
    "        Q[?current_action?,?current_state?] = ??? (reward + ??? - Q[?current_action?,?current_state?])\n",
    "        \n",
    "        # update variables\n",
    "        state = next_state #the new state becomes the old state\n",
    "        \n",
    "    #end of while loop\n",
    "#end of trial for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8dfee-64cb-439c-a3bf-9bc76bb76e1a",
   "metadata": {},
   "source": [
    "### Plotting performance\n",
    "\n",
    "Now that we have finished the simulation we should take closer look at the behavior of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ec4f6-308d-490b-be67-460788e6004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
    "\n",
    "xdata = range(nTrial)\n",
    "ydata = ???\n",
    "\n",
    "ax.plot(xdata, ydata, label = method)  # Plot some data on the axes.\n",
    "\n",
    "ax.set_title('Performance over time')\n",
    "ax.set_xlabel('Run')\n",
    "ax.set_ylabel('Number of moves')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f20dc-7f0d-4a74-9f6f-07bdcd1513af",
   "metadata": {},
   "source": [
    "### Additional plotting\n",
    "\n",
    "What else could be relevant for plotting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951eb53-3296-487b-927e-de3968bf7f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "095cfaa8-8a9d-492b-891d-511f18071d5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Questions\n",
    "\n",
    "How could we make the RL model converge faster to the best course of actions?\n",
    "- learning rule?\n",
    "- choice rule?\n",
    "- environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604252e5-ef35-4244-afe4-ebdbe90b3099",
   "metadata": {},
   "source": [
    "# Tips and tricks\n",
    "\n",
    "1. Print important variables using the `print()` function.  <br>\n",
    "   For example `if (iT % 10) == 0: print(iT)` prints the current trial every 10 trials\n",
    "        \n",
    "2. Or even better: plot it!  <br>\n",
    "   Import relevant packages to make use of advanced plotting option (e.g. matplotlib)\n",
    "   \n",
    "3. If you dont know a function, try the ? (e.g., `np.array?`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
